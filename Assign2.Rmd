---
title: Assignment 2
author: Clara Baudry & Gaja Nenadović & Hsuan Lee
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
geometry: margin=0.7in
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
library(tidyverse)
library(ggplot2)
library(lme4)
library(lmerTest)
```

# 1. Convert the wide data file into a long format. Check the data and recode if necessary.

```{r}
dat <- read.csv("curran_wide.csv")

# transform to long format, excluding sex and homeemo variables, and change time to int
dat_long <- pivot_longer(data=dat[,-c(10, 13)],
                         cols=c(2:9),
                         names_to = c(".value", "time"),
                         names_pattern = "(anti|read)(.)") %>% mutate_at(vars(time), as.integer)

# summary statistics
kable(summary(dat_long[,-1]))
```

We see that the *time* variable ranges from 1 to 4, which means that the first occasion is coded as 1. This means that the intercept cannot be interpreted meaningfully (the time variable will never be equal to 0). Therefore, we will recode the time variable and set its range from 0 to 3, so that the intercept is the predicted outcome for the first occasion when all other predictors are equal to 0. Looking at the ranges of the other predictors, we see that 0 is not a plausible value for any of them. This again means that the intercept will not be meaningful and we will have to extrapolate to interpret it. We will center the three predictors (*momage*, *read*, *homecog*), so that the intercept can be interpreted as the predicted outcome at the first occasion for a person with average values on all the other predictors.

```{r}
# rescale the time variable
dat_long$time <- dat_long$time - 1

# center the three continous predictors
dat_long[,-c(1, 4, 5)] <- scale(dat_long[,-c(1, 4, 5)], scale=F)

# check summary statistics again
kable(summary(dat_long[,-1]))
```

As we see from the table, 0 is now a plausible (and meaningful) value for every predictor.

## Check the linearity assumption, report and include plots.

1) If the outcome variable has linear relationship with time, that is, we should test if *antisocial behavior*(outcome variable) shows the linearity with *time* in our case.
2) If the outcome variable has linear relationship with the predictors. Therefore, we need to check the linear relationship between outcome variable and all the predictors in both level 1 and level 2, which are linear relationship between *antisocial behavior* and *reading recognition skills* for level 1; *antisocial behavior* and *cognitive stimulation*, *antisocial behavior* and *mother’s age* for level 2.

```{r}
# a scatterplot with a linear and quadratic trend for each of the time-varying predictors and the outcome

ggplot(dat_long,
       aes(x = time, y = anti)) +
       geom_point() +
       geom_smooth(method = "lm",
                   aes(color = "linear"),
                   se = FALSE) +
       geom_smooth(method = "lm",
                   formula = y ~ x + I(x**2),
                   aes(color = "quadratic"),
                   se = FALSE) +
       theme_minimal() +
       xlab("Time point") +
       ylab("Behaviour Problems Index")

ggplot(dat_long,
       aes(x = read, y = anti)) +
       geom_point() +
       geom_smooth(method = "lm",
                   aes(color = "linear"),
                   se = FALSE) +
       geom_smooth(method = "lm",
                   formula = y ~ x + I(x**2),
                   aes(color = "quadratic"),
                   se = FALSE) +
       theme_minimal() +
       xlab("Reading recognition score") +
       ylab("Behaviour Problems Index")


# the plot anti~time seprately for different persons
ggplot(data=dat_long[which(dat_long$id <= 900),], aes(x=time,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))


ggplot(data=dat_long[which(dat_long$id > 900 & dat_long$id <= 2000),], aes(x=time,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))

# the plot anti~read separately for different persons
ggplot(data=dat_long[which(dat_long$id <= 900),], aes(x=read,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))


ggplot(data=dat_long[which(dat_long$id > 900 & dat_long$id <= 2000),], aes(x=read,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))

# these plots have been added (check the 4 plots above)
#xyplot(anti ~ read|dat_long$id, dat_long[dat_long$id==34,],
#       grid = TRUE,
#       scales = list(x = list(log = 10, equispaced.log = FALSE)),
#       type = c("p", "r"), col.line = "darkorange", lwd = 3,
#       main = "")
### WORK ON THIS!
```

**but is the relation really linear between read and anti?**

The above plot reveals that both linear line and quadratic line are similar to each other, signifying that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *time*. Therefore, we could choose linear model for our followed analysis.

As we can see, the linear and quadratic lines of the *reading recognition skills* and *antisocial behavior* display a bit deviance, and the quadratic line seems have a better fit to the distribution of the scatter, meaning that quadratic model explain more variance than linear. However, the difference is not too much, we can therefore use linear line for our analysis.

Comparing the relation between *time* and *antisocial behaviour* between different children, we see that there is little variation. The same holds for the relation between *reading recognition skills* and *antisocial behaviour*

```{r}
# a scatterplot for each of the time invariant predictors and the aggregated outcome:

# antisocial behavior with cognitive stimulation
p2 <- dat_long %>% 
  group_by(id) %>% 
  mutate(antiAGR = mean(anti)) %>% 
  ggplot(aes(x = homecog, y = antiAGR)) +
    geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  theme_minimal()+
  xlab("") +
  ylab("")+
  ggtitle("Relation between homecog and aggregted anti")+
  theme(plot.title = element_text(size = 10, hjust=0.5))


# level 2
# antisocial behavior with cognitive stimulation
ggplot(data = dat_long,
aes(x = homecog, y = anti)) +
  geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
geom_smooth(method = "lm",
            formula = y ~ x + I(x^2),
            aes(color = "quadratic"),
            se = FALSE)
```

The above plot reveals that both linear line and quadratic line are similar to each other, signifying that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *cognitive stimulation*. Therefore, we could choose linear model for our followed analysis.

```{r}
# antisocial behavior with mother’s age.
p3 <- dat_long %>% 
  group_by(id) %>% 
  mutate(antiAGR = mean(anti)) %>% 
  ggplot(aes(x = momage, y = antiAGR)) +
    geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  theme_minimal()+
  xlab("") +
  ylab("")+
  ggtitle("Relation between momage and aggregted anti")+
  theme(plot.title = element_text(size = 10, hjust=0.5))


# level 2
# antisocial behavior with mother’s age.
ggplot(data = dat_long,
aes(x = momage, y = anti)) +
  geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
geom_smooth(method = "lm",
            formula = y ~ x + I(x**2),
            aes(color = "quadratic"),
            se = FALSE)
```

It could be seen that the difference between linear and quadratic lines are little, which means that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *mother’s age*. Therefore, we could choose linear model for our followed analysis.

## Check for outliers, report.

From the scatter plots we can observe that there are few outliers. However, since there are 221 observations in our data, these few higher score is acceptable. Therefore, we will keep these higher score in the followed analysis.

# 2. Answer the question: should you perform a multilevel analysis?

## What is the mixed model equation?

Equation of level 1:

$$Anti_{ti} = π_{0i} + e_{ti}$$

Equation of level 2:

$$π_{0i} = β_{00} + u_{0i}$$

Mixed Model Equation :

$$Anti_{ti} = β_{00} + u_{0i} + e_{ti}$$

## Provide and interpret the relevant results.

```{r}
# intercept-only model
m0 <- lm(anti ~ 1, data=dat_long)
summary(m0)


# intercept-only model with the random intercept term
m1 <- lmer(anti ~ 1 + (1|id), REML=F, data=dat_long)
summary(m1)


# comparison of m0 and m1
anova(m1, m0)
```

Model 1 fits significantly better than model 0, $χ^2(1) = 231.97$, $p<.001$. This points to the significant differences in the intercept (average *Behavior Problems Index*) between children ($\sigma_{u0}^2 > 0$).

## What is the intraclass correlation?

The formula for ICC is:

$ρ = \frac{σ^2_{u0}}{(σ^2_e + σ^2_{u0})} = \frac{1.579}{ (1.579 + 1.741)} = 0.48$.

```{r}
ICC <- 1.579/(1.579+1.741)
ICC
```

Intraclass correlation equals 0.48. We see that the proportion of the total unexplained variance accounted for by the individual-level differences is quite high.

## What is your conclusion regarding the overall question regarding the necessity of performing a multilevel analysis?

We have shown that the data has a multilevel structure:

- $\sigma_{u0}^2 > 0$: accounting for the variation in the *antisocial behavior* across children by adding the random intercept term significantly improves the model;

- the observations are not independent: the correlation of *anti* score on two occasions for the same child is considerably large, 0.48. 

Therefore, we should account for this dependence and variation by adopting the multilevel analysis.


# 3. Add the time-varying predictor(s).

We will proceed with adding time-varying predictors in two steps:

- create the model 2 with the fixed predictor *time*. This model will be used in further analysis as a benchmark to calculate $R^2$ on level 1 and level 2;

- create the model 3 by adding the fixed predictor *read*. 

```{r}
# add time as a fixed level 1 predictor
m2 <- lmer(anti ~ 1 + time + (1|id), REML=F, data=dat_long)
summary(m2)

# compare it with the random intercept-only model
anova(m2, m1)

# add read as a fixed level 1 predictor
m3 <- lmer(anti ~ 1 + time + read + (1|id), REML=F, data=dat_long)
summary(m3)

# compare the model with both level 1 predictors with the model with just time
anova(m3, m2)
```

## Provide and interpret the relevant results and provide your overall conclusion.

**Model 2 with fixed time**

The intercept is statistically significant, $\beta_{00} = 1.55, t(400) = 13.872, p < .001$. It can be interpreted as the average Behavior Problems Index at the first occasion. The coefficient for *time* is also significant, $\beta_{time}=0.18, t(683) = 4.513, p <.001$. It means that with each one point increase in time, that is with each measurement occasion, the expected *antisocial behavior* score increases by 0.18.

Comparing the model with *time* and the random intercept-only model, we see that the former fits the data better, $\chi^2(1)=20.06, p < .001$. AIC also indicates that model 2 ($AIC=3325.5$) is better than model 1 ($AIC=3343.5$).

**Model 3 with fixed time and reading recognition**

Both the intercept ($β_{00}=1.50, t(580) = 9.94, p <.001$) and the coefficient of *time* ($β_{time}=0.21, t(882) = 2.73, p < .01$) are again significant, but their values are slightly higher. The intercept now represents the Behavior Problems Index that would be expected at the first measurement occasion when the *reading recognition* score at that occasion is average. The coefficient for *time* can be interpreted as follows: with each measurement occasion, the predicted *antisocial behavior* score goes up by .21 when *read* is held constant. We see that *reading recognition skills* is not significant ($\beta_{read} = -0.03, t(831) = -0.54, p > .05$).

Comparing the model with *time* and the model with both *time* and *read*, we see that adding *read* does not improve the fit of the model, $\chi^2(1)=0.29, p > .05$. AIC values further confirm this, $AIC_{m2}=3325.5, AIC_{m3}=3327.3$. 

**Overall conclusion**

- We observed a significant linear time trend, where Behavior Problems Index increases with each measurement occasion. Furthermore, adding the *time* variable as a predictor improves the fit of the model.

- *Reading recognition skills* does not provide additional information in predicting *antisocial behavior* and does not improve the fit of the model. It should not be included as a fixed predictor in subsequent models.

The best of these two models is the one with only *time*, model 2. 
Its equation is:

$Anti_{ti} = β_{00} + β_{10} T_{ti} + u_{0i} + e_{ti}$


# 4. On which level or levels can you expect explained variance?
 
Generally, both level 1 and level 2 variance can be explained by the level 1 (time-varying) predictors. $R^2$ in the multilevel analysis of longitudinal data is calculated based on the reduction in unexplained variance components on each level compared to the model which includes only *time* instead of the random intercept-only model. This is due to the underestimation of variance on level 2 and, consequently, overestimation of variance on level 1 in the random intercept-only model, which render it inappropriate as a benchmark model.  

However, the only predictor that we added to the baseline model, that is *reading recognition skills*, is not significant, does not improve the model and will not be used in subsequent models as a fixed predictor. Therefore, we expect $R^2$ to be very low on both levels. 

## Calculate and interpret the explained variances.

The formula for the explained variance on level-1 is:

$R^2_{Level1} = \frac{σ^2_{e(model2)} - σ^2_{e(model3)}}{σ^2_{e(model2)}} = \frac{1.689 - 1.693}{1.689} = 0.00$

```{r}
(1.689 - 1.693) / 1.689
```

The formula for the explained variance on level-2 is:

$R^2_{Level2} = \frac{σ^2_{u0(model2)} - σ^2_{u0(model3)}}{σ^2_{u0(model2)}} = \frac{1.592 - 1.592}{1.592} = 0.01$

```{r}
(1.592 - 1.576) / 1.592
```

We see that explained variance on both level-1 ($R_{1}=-0.002$) and level-2($R_{2}=0.01$) is very low, once again confirming that the model 3 does not explain additional variance on any level and *read* should not be included as a fixed predictor in the subsequent models.
**negative explained variance -> why and not meaningful**


# 5. Add the time invariant predictor(s) to the model.

There are two variables at the child level, *mother’s age* and *cognitive stimulation*.

```{r}
# add momage and homecog as fixed level 2 predictors
m4a <- lmer(anti ~ 1 + time + momage + homecog + (1|id), REML=F, data=dat_long)
summary(m4a)

# compare the model with both level 2 predictors with the best model without them
anova(m4a, m2)
```

We notice that only the time-invariant variable *momage* is not a statistically significant predictor, $\beta_{momage}=-0.0009, t(221)=-0.02, p > .05$.

Comparing this model with the best model with only level 1 predictor, we see that the former model fits better, $\chi^2(2)=11.642, p < .01$. However, since we now added 2 parameters, this significant reduction in the deviance of the model is probably due to the significance of the other level 2 predictor, *homecog*. 

Therefore, we will create a new model, model 4b, without *momage*, and interpret the coefficients for this modified model. 

```{r}
# remove momage from the model and keep only homecog
m4b <- lmer(anti ~ 1 + time + homecog + (1|id), REML=F, data=dat_long)
summary(m4b)

# compare the model with only homecog with the model with both level 2 predictors:
anova(m4b, m4a)

# compare the model with homecog and the best model with only level 1 predictors
anova(m4b, m2)
```

## Provide and interpret the relevant results and provide your overall conclusion

As we see, the intercept is significant, $b_{0}=1.55, t(410) = 14.138, p <.001$. It represents the predicted Behavior Problems Index at the first occasion for a child who has an average *cognitive stimulation* score. 

Both predictors, that is *time*, $b_{time}=0.18, t(663) = 4.51, p <.001$, and *cognitive stimulation*, $b_{homecog}=-0.13, t(221) = -3.46, p < .001$, are significant. 

We can interpret them as follows: 

- with each one point increase in *time* (each measurement occasion), the expected *antisocial behavior* score goes up by 0.18, when *homecog* is kept constant;

- with each one unit increase in *cognitive stimulation*, the expected *anti* score at the first occasion decreases by 0.13. 

For the sake of comparison, we can also check if the model is influenced by leaving *momage* out of it. As we see, the model with both *homecog* and *momage* does not fit significantly better than the model with only *homecog*, $\chi^2(1)=0.00, p > .05$. Moreover, AIC values show that model 4b is better as well, $AIC_{m4a}=3317.8, AIC_{m4b}=3315.8$. 

Comparing the model 4b, and the best model with only level 1 predictors, model 2, we see that the former fits better, $\chi^2(1)=11.64, p < .001$. This model also has a lower AIC, $AIC_{m4b}=3315.8$, than model 2, $AIC_{m2}=3325.5$.

**Overall conclusion**

*Cognitive stimulation* is an important predictor of *antisocial behavior* on the child level and will be kept in the model. It explains a part of the differences between children at the first occasion. Conversely, *mother's age* does not provide much additional information in predicting these differences and should not be used as a fixed predictor in subsequent analysis (on its own, without the interaction). 

# 6. On which level or levels can you expect explained variance?

Since we have added only a level 2 predictor, which predicts the intercept for each child, we expect explained variance only on this level (that is, we expect it to explain only the variance of the intercept across children).

## Calculate and interpret the explained variances.

The formula for the explained variances of level-2 is:

$R^2_{Level2} = \frac{σ^2_{u0(model2)} - σ^2_{u0(model4a)}}{σ^2_{u0(model2)}} = \frac{1.592 - 1.592}{1.592} = 0.01$

```{r}
(1.592 - 1.488) / 1.592
```

The proportion of explained variance on level-2 is .07. *Cognitive stimulation* can explain 7% of the variance in individuals' *antisocial behavior* at the first occasion.

# 7. For the time-varying predictor(s), check if the slope is fixed or random.

We will check if the slope is random for each of the time-varying predictors separately.
Firstly, we should inspect the random slope for *time* and modify the model based on the results. Secondly, even though *reading recognition* was not a significant predictor of *antisocial behavior* when treated as fixed across children, it is possible that its slope varies across children and this variation might be non-negligible. Therefore, we will check if the slope of *read* is random as well. 

## What are the null- and alternative hypotheses?

- For *time*:

**The null hypothesis is that *time* does not have the random slope. The alternative *time* has the random slope.** I don't think we have to add the hypotheses in words, since it's the same as in symbols

$$H_0: σ^2_{u_1}=0$$

$$H_1: σ^2_{u_1}>0$$

- For *reading recognition*:

**The null hypothesis is that *reading recognition* does not have the random slope. The alternative *reading recognition* has the random slope.** same here

$$H_0: σ^2_{u_2}=0$$

$$H_1: σ^2_{u_2}>0$$

## Provide and interpret the relevant results.

```{r}
# add the random slope term for the time variable
m5a <- lmer(anti ~ 1 + time + homecog + (1 + time|id), REML=F, data=dat_long)

summary(m5a)

# compare the time random-slope model with the best fixed-slope model
anova(m5a, m4b)

# add the random slope term for the read variable
m5b <- lmer(anti ~ 1 + time + homecog + read + (1 + time + read|id), REML=F, data=dat_long)


summary(m5b)

# compare the random-slope model with the best fixed-slope model
anova(m5b, m5a)
```

**Model 5a with random slope for time**

The intercept is significant, $b_0=1.55, t(219)=16.26, p<.001$, and can be interpreted as the expected outcome at the first occasion for a child with average *homecog* score. 

The coefficient for *time* is significant too, $b_{time}=0.18, t(221)=4.14, p<.001$. It shows that with each occasion, predicted Behavior Problems Index increases by 0.18 when *homecog* is kept constant. However, since we now have a varying slope for *time*, this coefficient is just the expected value across all children.

Lastly, the coefficient for *homecog* is significant as well, $b_{homecog}=-0.10, t(221)=-2.79, p<.01$. With each one point increase in *cognitive stimulation*, the expected *anti* score at the first occasion goes down by 0.1.

*+covariance interpretation+changes in variance components+coeff*

Comparing this model with the model without the random slope, we see that the former fits the data significantly better, $χ^2(2) = 26.56$, $p<.001$, and its AIC is lower, $AIC_{m5a}=3293.3, AIC_{m4b}=3315.8$. This shows that adding the random slopes term and the covariance between the slope and the intercept improves the model.

**Model 5b with random slope for time and read**

The intercept and coefficients for *time* and *homecog* are still significant, with slightly different values. Their interpretation remains the same as in the previous model.
*Read* is not a significant predictor, as expected, since its coefficient is now just the average value across all children, which has already been shown as not predictive of *antisocial behavior* in model 3. 

*covariance, changes in var*

We see that adding, besides the coefficient for *read*, the random slope term, covariance between slope for *time* and slope for *read*, as well as the covariance between the intercept and slope for *read* to the model, does not reduce its deviance, $χ^2(4) = 0.2609$, $p>.05$.

*as seen from the plots it doesn't vary*

## Provide an overall conclusion.

We can conclude that there is variation in the relation between time and *antisocial behavior* across children and we should account for it by allowing the random slope for *time*. *Read* does not have a varying slope and including it in the model, together with the other parameter, does not reduce its deviance significantly. 

One thing to note here is that the hypotheses cannot be assessed directly with the test we have used. Since there are 2 additional parameters in m5a, $\sigma^2_{u1}$ and $cov(u1, uo)$, significant $\chi^2$ test does not necessarily reflect significance of $\sigma^2_{u1}$. But even if only the covariance parameter is significant, the variance of the slope is implied. Similarly, there are 4 additional parameters in m5b and ...


# 8. If there is a random slope, set up a model that predicts the slope variation.

```{r}
# add the predictor of the slope for the time variable
m6a <- lmer(anti ~ 1 + time + homecog + time*homecog + (1 + time|id), REML=F, data=dat_long)

summary(m6a) # homecog is not significant anymore, but the interaction is


# compare the model with the predictor of the slope for time with the model without the predictor
anova(m6a, m5)

m6b <- lmer(anti ~ 1 + time + time*momage + time*homecog + (1 + time|id), REML=F, data=dat_long)

summary(m6b)

anova(m6b, m6a)
```

## Provide and interpret the relevant results and provide your overall conclusion.

**Model 6a**

**Model 6b**

**Overall conclusion**

# 9. Decide on a final model.

## provide the separate level 1 and 2 model equations, as well as the mixed model equation.

## Check the normality assumption for both the level-1 and level-2 errors, report.

```{r}
# level 1:
qqnorm(residuals(m6), main = "Q-Q plot for the first level residuals")
qqline(residuals(m6))

# level 2:

# intercept
qqnorm(ranef(m6)$id[,1], main = "Q-Q plot for the intercept")
qqline(ranef(m6)$id[,1])

# slope
qqnorm(ranef(m6)$id[,2], main = "Q-Q plot for the slope of time variable")
qqline(ranef(m6)$id[,2])
```


# I temporarily moved the equations here

## Question 3

The equation for model_2 and model_3 are:

model_2 (baseline model):

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + u_{0i}$

$π_{1i} = β_{10}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{10} T_{ti} + u_{0i} + e_{ti}$

model_3 (time-varying predictors):

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + π_{2i}X_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + u_{0i}$

$π_{1i} = β_{10}$

$π_{2i} = β_{20}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{10}T_{ti} + β_{20}X_{ti} + u_{0i} + e_{ti}$

## Question 5

The equation to model_4 is:

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + β_{01}momage + β_{02}homecog + u_{0i}$

$π_{1i} = β_{10}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{01}momage + β_{02}homecog + β_{10}T_{ti} + u_{0i} + e_{ti}$

## Question 7

The equation of model_5a is:

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + β_{02}homecog + u_{0i}$

$π_{1i} = β_{10} + u_{1i}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{02}homecog + β_{10}T_{ti} + u_{0i} + u_{1i}T_{ti} + e_{ti}$

