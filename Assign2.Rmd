---
title: Assignment 2
author: Clara Baudry & Gaja Nenadović & Hsuan Lee
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
geometry: margin=0.7in
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(knitr)
library(tidyverse)
library(ggplot2)
library(lme4)
library(lmerTest)
```

# 1. Convert the wide data file into a long format. Check the data and recode if necessary.

```{r}
dat <- read.csv("curran_wide.csv")

# transform to long format, excluding sex and homeemo variables, and change time to int
dat_long <- pivot_longer(data=dat[,-c(10, 13)],
                         cols=c(2:9),
                         names_to = c(".value", "time"),
                         names_pattern = "(anti|read)(.)") %>% mutate_at(vars(time), as.integer)

# summary statistics
kable(summary(dat_long[,-1]))
```
We see that the *time* variable ranges from 1 to 4, which means that the first occasion is coded as 1. This means that the intercept cannot be interpreted meaningfully (the time variable will never be equal to 0). Therefore, we will recode the time variable and set its range from 0 to 3, so that the intercept is the predicted outcome for the first occasion when all other predictors are equal to 0. Looking at the ranges of the other predictors, we see that 0 is not a plausible value for any of them. This again means that the intercept will not be meaningful and we will have to extrapolate to interpret it. We will center the three predictors (*momage*, *read*, *homecog*), so that the intercept can be interpreted as the predicted outcome at the first occasion for a person with average values on all the other predictors.

```{r}
# rescale the time variable
dat_long$time <- dat_long$time - 1

# center the three continous predictors
dat_long[,-c(1, 4, 5)] <- scale(dat_long[,-c(1, 4, 5)], scale=F)

# check summary statistics again
kable(summary(dat_long[,-1]))
```

As we see from the table, 0 is now a plausible (and meaningful) value for every predictor.

## Check the linearity assumption, report and include plots.


1) If the outcome variable has linear relationship with time, that is, we should test if *antisocial behavior*(outcome variable) shows the linearity with *time* in our case.
2) If the outcome variable has linear relationship with the predictors. Therefore, we need to check the linear relationship between outcome variable and all the predictors in both level 1 and level 2, which are linear relationship between *antisocial behavior* and *reading recognition skills* for level 1; *antisocial behavior* and *cognitive stimulation*, *antisocial behavior* and *mother’s age* for level 2.


```{r}
# a scatterplot with a linear and quadratic trend for each of the time-varying predictors and the outcome

ggplot(dat_long,
       aes(x = time, y = anti)) +
       geom_point() +
       geom_smooth(method = "lm",
                   aes(color = "linear"),
                   se = FALSE) +
       geom_smooth(method = "lm",
                   formula = y ~ x + I(x**2),
                   aes(color = "quadratic"),
                   se = FALSE) +
       theme_minimal() +
       xlab("Time point") +
       ylab("Behaviour Problems Index")

ggplot(dat_long,
       aes(x = read, y = anti)) +
       geom_point() +
       geom_smooth(method = "lm",
                   aes(color = "linear"),
                   se = FALSE) +
       geom_smooth(method = "lm",
                   formula = y ~ x + I(x**2),
                   aes(color = "quadratic"),
                   se = FALSE) +
       theme_minimal() +
       xlab("Reading recognition score") +
       ylab("Behaviour Problems Index")


# the plot anti~time seprately for different persons
ggplot(data=dat_long[which(dat_long$id <= 900),], aes(x=time,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))


ggplot(data=dat_long[which(dat_long$id > 900 & dat_long$id <= 2000),], aes(x=time,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))

# the plot anti~read seprately for different persons
ggplot(data=dat_long[which(dat_long$id <= 900),], aes(x=read,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))


ggplot(data=dat_long[which(dat_long$id > 900 & dat_long$id <= 2000),], aes(x=read,y=anti)) +
  geom_point() +
  stat_smooth(method="lm", fullrange=TRUE) +
  xlab("Time point") + ylab("Behaviour Problems Index") + 
  facet_wrap( ~ id) +
  theme(axis.title=element_text(size=16),
        axis.text=element_text(size=14),
        strip.text=element_text(size=14))

# these plots have been added (check the 4 plots above)
#xyplot(anti ~ read|dat_long$id, dat_long[dat_long$id==34,],
#       grid = TRUE,
#       scales = list(x = list(log = 10, equispaced.log = FALSE)),
#       type = c("p", "r"), col.line = "darkorange", lwd = 3,
#       main = "")
### WORK ON THIS!
```

**but is the relation really linear between read and anti?**

The above plot reveals that both linear line and quadratic line are similar to each other, signifying that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *time*. Therefore, we could choose linear model for our followed analysis.

As we can see, the linear and quadratic lines of the *reading recognition skills* and *antisocial behavior* display a bit deviance, and the quadratic line seems have a better fit to the distribution of the scatter, meaning that quadratic model explain more variance than linear. However, the difference is not too much, we can therefore use linear line for our analysis.

Comparing the relation between *time* and *antisocial behaviour* between different children, we see that there is little variation. The same holds for the relation between *reading recognition skills* and *antisocial behaviour*

```{r}
# a scatterplot for each of the time invariant predictors and the aggregated outcome:

# antisocial behavior with cognitive stimulation
p2 <- dat_long %>% 
  group_by(id) %>% 
  mutate(antiAGR = mean(anti)) %>% 
  ggplot(aes(x = homecog, y = antiAGR)) +
    geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  theme_minimal()+
  xlab("") +
  ylab("")+
  ggtitle("Relation between homecog and aggregted anti")+
  theme(plot.title = element_text(size = 10, hjust=0.5))


# level 2
# antisocial behavior with cognitive stimulation
ggplot(data = dat_long,
aes(x = homecog, y = anti)) +
  geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
geom_smooth(method = "lm",
            formula = y ~ x + I(x^2),
            aes(color = "quadratic"),
            se = FALSE)
```

The above plot reveals that both linear line and quadratic line are similar to each other, signifying that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *cognitive stimulation*. Therefore, we could choose linear model for our followed analysis.

```{r}
# antisocial behavior with mother’s age.
p3 <- dat_long %>% 
  group_by(id) %>% 
  mutate(antiAGR = mean(anti)) %>% 
  ggplot(aes(x = momage, y = antiAGR)) +
    geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
  geom_smooth(method = "lm",
              formula = y ~ x + I(x^2),
              aes(color = "quadratic"),
              se = FALSE) +
  theme_minimal()+
  xlab("") +
  ylab("")+
  ggtitle("Relation between momage and aggregted anti")+
  theme(plot.title = element_text(size = 10, hjust=0.5))


# level 2
# antisocial behavior with mother’s age.
ggplot(data = dat_long,
aes(x = momage, y = anti)) +
  geom_point() +
  geom_smooth(method = "lm",
              aes(color = "linear"),
              se = FALSE) +
geom_smooth(method = "lm",
            formula = y ~ x + I(x**2),
            aes(color = "quadratic"),
            se = FALSE)
```

It could be seen that the difference between linear and quadratic lines are little, which means that the variance explanation's extent of linear and quadratic are almost the same for the relationship between *antisocial behavior* and *mother’s age*. Therefore, we could choose linear model for our followed analysis.

## Check for outliers, report.
From the scatter plots we can observe that there are few outliers. However, since there are 221 observations in our data, these few higher score is acceptable. Therefore, we will keep these higher score in the followed analysis.


# 2. Answer the question: should you perform a multilevel analysis?


## What is the mixed model equation?

Equation of level 1:

$Anti_{ti} = π_{0i} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + u_{0i}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + u_{0i} + e_{ti}$

## Provide and interpret the relevant results.

```{r}
# intercept-only model
m0 <- lm(anti ~ 1, data=dat_long)
summary(m0)


# intercept-only model with the random intercept term
m1 <- lmer(anti ~ 1 + (1|id), REML=F, data=dat_long)
summary(m1)


# comparison of m0 and m1
anova(m1, m0)
```

Model 1 fits significantly better than model 0, $χ^2(1) = 231.97$, $p<.001$. This points to the significant differences in the intercept (average *Behavior Problems Index*) between children ($\sigma_{u0}^2 > 0$).

## What is the intraclass correlation?

The formula for ICC is:

$ρ = \frac{σ^2_{u0}}{(σ^2_e + σ^2_{u0})} = \frac{1.579}{ (1.579 + 1.741)} = 0.48$.

```{r}
ICC <- 1.579/(1.579+1.741)
ICC
```

Intraclass correlation equals 0.48. We see that the proportion of the total unexplained variance accounted for by the individual-level differnces is quite high.

## What is your conclusion regarding the overall question regarding the necessity of performing a multilevel analysis?

We have shown that the data has a multilevel structure:

- $\sigma_{u0}^2 > 0$: accounting for the variation in the *antisocial behavior* across children by adding the random intercept term significantly improves the model;

- the observations are not independent: the correlation of *anti* score on two occasions for the same child is considerably large, 0.48. 

Therefore, we should account for this dependence and variation by adopting the multilevel analysis.


# 3. Add the time-varying predictor(s).

Three steps will be implemented here. The first one is to create the model 2 with the fixed level 1 variable *time* as a baseline model. We will use it in further analysis to calculate $R^2$ values for subsequent models. The second step is to create model 3, adding the *fixed time-varying level 1 predictor read*, and check if it is a significant predictor. 

The equation for model_2 and model_3 are:

model_2 (baseline model):

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + u_{0i}$

$π_{1i} = β_{10}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{10} T_{ti} + u_{0i} + e_{ti}$

model_3 (time-varying predictors):

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + π_{2i}X_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + u_{0i}$

$π_{1i} = β_{10}$

$π_{2i} = β_{20}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{10}T_{ti} + β_{20}X_{ti} + u_{0i} + e_{ti}$

```{r}
# add time as a fixed level 1 predictor
m2 <- lmer(anti ~ 1 + time + (1|id), REML=F, data=dat_long)
summary(m2)

# compare it with the random intercept-only model
anova(m2, m1)

# add read as a fixed level 1 predictor
m3 <- lmer(anti ~ 1 + time + read + (1|id), REML=F, data=dat_long)
summary(m3)

# compare the model with both level 1 predictors with the model with just time var
anova(m3, m2)
```

## Provide and interpret the relevant results and provide your overall conclusion.

The intercept in model 2 with fixed *time* is statistically significant, $\beta_{00} = 1.38, t(400) = 13.872, p < .001$. It can be interpreted as the average Behaviour Problems Index at the first occasion. The coefficient for *time* is also significant, $\beta_{time}=0.18, t(683) = 4.513, p <.001$. It means that with each one point increase in time, that is with each measurement occasion, the expected *antisocial behavior* score increases by 0.18.

Comparing the model with *time* as a predictor and the random intercept-only model, we see that the former fits the data better, $\chi(1)^2=20.06, p < .001$.
**add AIC**

In model 3, we add the fixed time-varying predictor *read*. Both the intercept ($β_{00}=1.50, t(580) = 9.94, p <.001$) and the coefficient of *time* ($β_{time}=0.21, t(882) = 2.729, p < .01$) are again significant. The intercept can be now interpreted as the predicted *anti* score for the first occasion for a person with average *read* score. The coefficient for *time* can be interpreted as follows: with each measurement occasion, the predicted *anti* score goes up by .21 when *read* is held constant. We see that *reading recognition skills*($\beta_{read}$) is not significant ($\beta_{read} = -0.0337, t(831) = -0.542, p > .05$). We can conclude that it does not provide additional information in predicting *antisocial behavior*. Therefore, we should drop it out of the model. 

Comparing the model with *time* and the model with both *time* and *read*, we see that adding *read* to the model does not improve its fit, $\chi^2(1)=0.29, p > .05$.

$Anti_{ti} = β_{00} + β_{10} T_{ti} + u_{0i} + e_{ti}$


# 4. On which level or levels can you expect explained variance?

Before the analysis, we expect that the time-varying predictors may explain variance on each of the two levels.

Baseline: model with time


## Calculate and interpret the explained variances.
**a. Calculate and interpret the explained variances.**

Generally, both level 1 and level 2 variance can be explained by the level 1 (time-varying) predictors. However, the only level 1 predictor that we added to the baseline model, that is *reading recognition skills*, is not significant and is eliminated from the model. Therefore, we can informatively compute $R^2$ for the model with *read*, but we expect it to be low for both levels. 


Not sure what this means?
**In general situation, both level-1 and level-2 explained variance are important, however, since the level 2 predictor *reading recognition skills* does not improve the model, so we eliminate it from the model, in our case, there is no longer a level 2 predictor anymore, base on this perspective, plus the level-1 predictor *time* is significant, and keep in the model, we therefore expect the explained variance of level-1. However, we will also inspect the explained variances in level-2.**

The formula for the explained variances of level-1 is:

$R^2_{Level1} = \frac{σ^2_{e(model2)} - σ^2_{e(model3)}}{σ^2_{e(model2)}} = \frac{1.689 - 1.693}{1.689} = 0.00$

```{r}
(1.689 - 1.693) / 1.689
```

The formula for the explained variances of level-2 is:

$R^2_{Level2} = \frac{σ^2_{u0(model2)} - σ^2_{u0(model3)}}{σ^2_{u0(model2)}} = \frac{1.592 - 1.592}{1.592} = 0.01$
```{r}
(1.592 - 1.576) / 1.592
```

We see that both level-1 ($R_{1}=-0.002$) and level-2($R_{2}=0.01$) are very low, once again confirming that the model 3 does not explain additional variance on any level and *read* should not be included as a fixed predictor in the subsequent models.


# 5. Add the time invariant predictor(s) to the model.
There are two predictors on level 2, *mother’s age* and *cognitive stimulation*. 

The equation to model_4 is:

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + β_{01}momage + β_{02}homecog + u_{0i}$

$π_{1i} = β_{10}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{01}momage + β_{02}homecog + β_{10}T_{ti} + u_{0i} + e_{ti}$

```{r}
# add momage and homecog as fixed level 2 predictors
m4a <- lmer(anti ~ 1 + time + momage + homecog + (1|id), REML=F, data=dat_long)
summary(m4a)

# compare the model with both level 2 predictors with the best model without them
anova(m4a, m2)
```

We notice that only the time-invariant variable *momage* is not a statistically significant predictor, $\beta_{momage}=-0.0009, t(221)=-0.02, p > .05$.

Comparing the model with *momage* with the best model with only level 1 predictors, we see that the former model fits better, $\chi^2(2)=11.642, p < .01$, but this is due to the significance of the second level 2 predictor, *homecog*. 

Therefore, we will create a new model, model 4b, without *momage*, and interpret the coefficients for this modified model. 

```{r}
# remove momage from the model and keep only homecog
m4b <- lmer(anti ~ 1 + time + homecog + (1|id), REML=F, data=dat_long)
summary(m4b)

# compare the model with only homecog with the model with both level 2 predictors:
anova(m4b, m4a)

# compare the model with homecog and the best model with only level 1 predictors
anova(m4b, m2)
```


## Provide and interpret the relevant results and provide your overall conclusion

As we see, the intercept is again significant, $\beta_{00}=1.55, t(410) = 14.138, p <.001$. It represents the predicted value for the *anti* variable at the first occasion for a child who received average *cognitive stimulation*. 

Both predictors, that is *time*, $\beta_{time}=0.18, t(663) = 4.51, p <.001$, and *cognitive stimulation*, $\beta_{homecog}=-0.13, t(221) = -3.46, p < .001$, are significant. 

We can interpret them as follows: 
- with each one point increase in *time* (each measurement occasion), the expected *antisocial behavior* score goes up by 0.18, when *homecog* is kept constant;
- with each one unit increase in *cognitive stimulation*, the expected *anti* score decreases by 0.13, when *time* is kept constant. 

The model with both *homecog* and *momage* does not fit significantly better than the model with only *homecog*, $\chi^2(1)=0.00, p > .05$. 

Comparing the better of these two models, that is model 4b with only *homecog*, however, fits better than the best model with only level 1 predictors (model 2), $\chi^2(1)=11.64, p < .001$. 

We conclude that only *homecog* is an important predictor of the differences in *anti* between the children and should be kept in the model. 

*predictor of intercept* explains the differences between the children on the first measurement occasion.

# 6. On which level or levels can you expect explained variance?

Since we have added only a level 2 predictor, we expect the explained variance only on this level.

## Calculate and interpret the explained variances.
The formula for the explained variances of level-2 is:

$R^2_{Level2} = \frac{σ^2_{u0(model2)} - σ^2_{u0(model4a)}}{σ^2_{u0(model2)}} = \frac{1.592 - 1.592}{1.592} = 0.01$
```{r}
(1.592 - 1.488) / 1.592
```

In conclusion, the explained variance of level-2($R^2 = .07$) is .07, indicating that the adding level-2 predictor *cognitive stimulation* can explain 7% of the variance between individuals' *antisocial behavior*.


# 7. For the time-varying predictor(s), check if the slope is fixed or random.

## What are the null- and alternative hypotheses?

Two things we should check here, the first is that we should inspect the random slope for *time*; the second is that even though the time-varying variable *reading recognition* shows insignificant in the previous model, it can still have random slope, hence, we should take variable *read* back to the model for testing the random slope. Therefore, we will create two models to each, and there would be two null hypothesis:

For *time*:

The null hypothesis is that *time* does not have the random slope. The alternative *time* has the random slope. 

$H_0: σ^2_{u_1}=0$

$H_1: σ^2_{u_1}≠0$ 

For *reading recognition*:

The null hypothesis is that *reading recognition* does not have the random slope. The alternative *reading recognition* has the random slope. 

$H_0: σ^2_{u_2}=0$

$H_1: σ^2_{u_2}≠0$ 

## Provide and interpret the relevant results.

```{r}
# add the random slope term for the time variable
m5 <- lmer(anti ~ 1 + time + homecog + (1 + time|id), REML=F, data=dat_long)
summary(m5)

# compare the random-slope model with the best fixed-slope model
anova(m5, m4b)

# we should keep the random slope term
```

## Provide an overall conclusion.


# 8. If there is a random slope, set up a model that predicts the slope variation.

```{r}
m6_mom <- lmer(anti ~ 1 + time + homecog + time*momage + time*homecog + (1 + time|id), REML=F, data=dat_long)

summary(m6_mom)

# add the predictor of the slope for the time variable
m6 <- lmer(anti ~ 1 + time + homecog + time*homecog + (1 + time|id), REML=F, data=dat_long)

summary(m6) # homecog is not significant anymore, but the interaction is

# compare the model with the predictor of the slope for time with the model without the predictor
anova(m6, m5)
```

## Provide and interpret the relevant results and provide your overall conclusion.
momage not singificant
time*momage not significant
homecog not significant
time*homecog 

# 9. Decide on a final model.

## provide the separate level 1 and 2 model equations, as well as the mixed model equation.

## Check the normality assumption for both the level-1 and level-2 errors, report.

```{r}
# level 1:
qqnorm(residuals(m6), main = "Q-Q plot for the first level residuals")
qqline(residuals(m6))

#qqline(residuals(m6))


# level 2:

# intercept
qqnorm(ranef(m6)$id[,1], main = "Q-Q plot for the intercept")
qqline(ranef(m6)$id[,1])

#qqline(ranef(m6)$id[,1])

# slope
qqnorm(ranef(m6)$id[,2], main = "Q-Q plot for the slope of time variable")
qqline(ranef(m6)$id[,2])

#qqline(ranef(m6)$id[,2])
```


**equations**


###

The equation become:

Equation of level 1:

$Anti_{ti} = π_{0i} + π_{1i}T_{ti} + e_{ti}$

Equation of level 2:

$π_{0i} = β_{00} + β_{02}homecog + u_{0i}$

$π_{1i} = β_{10}$

Mixed Model Equation :

$Anti_{ti} = β_{00} + β_{02}homecog + β_{10}T_{ti} + u_{0i} + e_{ti}$
